{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import os, random, gc\n",
    "import numpy as np\n",
    "from src.fastai_fix import *\n",
    "import tifffile\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87228b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    path = 'data/'\n",
    "    out = 'experiments/init'\n",
    "    \n",
    "    num_workers = 12\n",
    "    seed = 2023\n",
    "    bs = 8\n",
    "    \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.seed)\n",
    "os.makedirs(CONFIG.out, exist_ok=True)\n",
    "\n",
    "def WrapperAdamW(param_groups,**kwargs):\n",
    "    return OptimWrapper(param_groups,torch.optim.AdamW)\n",
    "\n",
    "from src.radam import Over9000\n",
    "def WrapperOver9000(param_groups,**kwargs):\n",
    "    return OptimWrapper(param_groups,opt=Over9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ef89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2tensor(img,dtype:np.dtype=np.float32):\n",
    "    if img.ndim==2 : img = np.expand_dims(img,2)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    return torch.from_numpy(img.astype(dtype, copy=False))\n",
    "\n",
    "def get_aug():\n",
    "    return A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=30, p=0.75),\n",
    "        A.OneOf([\n",
    "            A.RandomGamma(gamma_limit=(50, 150), always_apply=True),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, always_apply=True),],p=0.5),\n",
    "        A.OneOf([\n",
    "            A.MotionBlur(always_apply=True),\n",
    "            A.GaussianBlur(always_apply=True),],p=0.25)\n",
    "    ], p=1)\n",
    "\n",
    "class ContrailsDataset(Dataset):\n",
    "    def __init__(self, path, train=True, tfms=None, repeat=1):\n",
    "        self.path = os.path.join(path, 'train_adj2' if train else 'val_adj2')\n",
    "        self.fnames = sorted([fname for fname in os.listdir(self.path) if \\\n",
    "                       fname.split('.')[0].split('_')[-1] == 'img'])\n",
    "        self.train, self.tfms = train, tfms\n",
    "        self.nc = 3\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.repeat*len(self.fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx%len(self.fnames)\n",
    "        img = tifffile.imread(os.path.join(self.path, self.fnames[idx]))\n",
    "        img = img.reshape(*img.shape[:2],self.nc,-1)[:,:,:,:5]\n",
    "        img = img.reshape(*img.shape[:2],-1)\n",
    "        mask = tifffile.imread(os.path.join(self.path, self.fnames[idx].replace('img','mask')))\n",
    "\n",
    "        if self.tfms is not None:\n",
    "            augmented = self.tfms(image=img,mask=mask)\n",
    "            img,mask = augmented['image'],augmented['mask']\n",
    "        \n",
    "        img = cv2.resize(img, (2*img.shape[1],2*img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "        img,mask = img2tensor(img/255),img2tensor(mask/255)\n",
    "        img = img.view(self.nc, -1, *img.shape[1:])\n",
    "        \n",
    "        return img,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae973775",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F_th(Metric):\n",
    "    def __init__(self, ths=np.arange(0.1,0.9,0.01), beta=1): \n",
    "        self.ths = ths\n",
    "        self.beta = beta\n",
    "        \n",
    "    def reset(self): \n",
    "        self.tp = torch.zeros(len(self.ths))\n",
    "        self.fp = torch.zeros(len(self.ths))\n",
    "        self.fn = torch.zeros(len(self.ths))\n",
    "        \n",
    "    def accumulate(self, learn):\n",
    "        pred,targ = flatten_check(torch.sigmoid(learn.pred.float()), \n",
    "                                  (learn.y > 0.5).float())\n",
    "        for i,th in enumerate(self.ths):\n",
    "            p = (pred > th).float()\n",
    "            self.tp[i] += (p*targ).float().sum().item()\n",
    "            self.fp[i] += (p*(1-targ)).float().sum().item()\n",
    "            self.fn[i] += ((1-p)*targ).float().sum().item()\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        self.dices = (1 + self.beta**2)*self.tp/\\\n",
    "            ((1 + self.beta**2)*self.tp + self.beta**2*self.fn + self.fp + 1e-6)\n",
    "        return self.dices.max()\n",
    "\n",
    "from src.lovasz import lovasz_hinge\n",
    "def loss_comb(x,y):\n",
    "    return F.binary_cross_entropy_with_logits(x,y) + \\\n",
    "        0.01*0.5*(lovasz_hinge(x,y,per_image=False) + lovasz_hinge(-x,1-y,per_image=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import OrderedDict\n",
    "from src.coat import CoaT,coat_lite_mini,coat_lite_small,coat_lite_medium\n",
    "    \n",
    "class LayerNorm2d(nn.Module):\n",
    "    def __init__(self, num_channels: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        u = x.mean(1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.eps)\n",
    "        x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "        return x\n",
    "    \n",
    "def icnr_init(x, scale=2, init=nn.init.kaiming_normal_):\n",
    "    \"ICNR init of `x`, with `scale` and `init` function\"\n",
    "    ni,nf,h,w = x.shape\n",
    "    ni2 = int(ni/(scale**2))\n",
    "    k = init(x.new_zeros([ni2,nf,h,w])).transpose(0, 1)\n",
    "    k = k.contiguous().view(ni2, nf, -1)\n",
    "    k = k.repeat(1, 1, scale**2)\n",
    "    return k.contiguous().view([nf,ni,h,w]).transpose(0, 1)\n",
    "\n",
    "class PixelShuffle_ICNR(nn.Sequential):\n",
    "    def __init__(self, ni, nf=None, scale=2, blur=True):\n",
    "        super().__init__()\n",
    "        nf = ni if nf is None else nf\n",
    "        layers = [nn.Conv2d(ni, nf*(scale**2), 1), LayerNorm2d(nf*(scale**2)), \n",
    "                  nn.GELU(), nn.PixelShuffle(scale)]\n",
    "        layers[0].weight.data.copy_(icnr_init(layers[0].weight.data))\n",
    "        if blur: layers += [nn.ReplicationPad2d((1,0,1,0)), nn.AvgPool2d(2, stride=1)]\n",
    "        super().__init__(*layers)\n",
    "    \n",
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels:list, output_channels:list):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "             nn.GELU(), LayerNorm2d(out_ch*2),\n",
    "             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "            for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "    def forward(self, xs:list, last_layer):\n",
    "        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n",
    "               for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "        hcs.append(last_layer)\n",
    "        return torch.cat(hcs, dim=1)\n",
    "\n",
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = LayerNorm2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(ni, nf, 3, padding=1),nn.GELU())\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(nf, nf, 3, padding=1),nn.GELU())\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "    def forward(self, up_in:torch.Tensor, left_in:torch.Tensor) -> torch.Tensor:\n",
    "        s = left_in\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))\n",
    "    \n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, up_in_c:int, nf:int=None, blur:bool=True,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        ni = up_in_c//4\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, ni, blur=blur, **kwargs)\n",
    "        nf = nf if nf is not None else max(up_in_c//4,16)\n",
    "        self.conv = nn.Sequential(nn.Conv2d(ni, ni, 3, padding=1),\n",
    "                                  LayerNorm2d(ni) if ni >= 16 else nn.Identity(),\n",
    "                                  nn.GELU(),nn.Conv2d(ni, nf, 1))\n",
    "\n",
    "    def forward(self, up_in:torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(self.shuf(up_in))\n",
    "    \n",
    "class LSTM_block(nn.Module):\n",
    "    def __init__(self, n, bidirectional=False, num_layers=1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n, n if not bidirectional else n//2, batch_first=True,\n",
    "                            bidirectional=bidirectional, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        s = x.shape\n",
    "        x = x.flatten(-2,-1).permute(0,3,1,2).flatten(0,1)\n",
    "        x = self.lstm(x)[0]\n",
    "        x = x.view(-1,s[3],s[4],s[1],s[2]).permute(0,3,4,1,2)\n",
    "        return x\n",
    "    \n",
    "class CoaT_ULSTM(nn.Module):\n",
    "    def __init__(self, pre='coat_lite_medium_a750cd63.pth', arch='medium', num_classes=1, ps=0, **kwargs):\n",
    "        super().__init__()\n",
    "        if arch == 'mini': \n",
    "            self.enc = coat_lite_mini(return_interm_layers=True)\n",
    "            nc = [64,128,320,512]\n",
    "        elif arch == 'small': \n",
    "            self.enc = coat_lite_small(return_interm_layers=True)\n",
    "            nc = [64,128,320,512]\n",
    "        elif arch == 'medium': \n",
    "            self.enc = coat_lite_medium(return_interm_layers=True)\n",
    "            nc = [128,256,320,512]\n",
    "        else: raise Exception('Unknown model') \n",
    "        \n",
    "        if pre is not None:\n",
    "            sd = torch.load(pre)['model']\n",
    "            print(self.enc.load_state_dict(sd,strict=False))\n",
    "        \n",
    "        self.lstm = nn.ModuleList([LSTM_block(nc[-2]),LSTM_block(nc[-1])])\n",
    "        self.dec4 = UnetBlock(nc[-1],nc[-2],384)\n",
    "        self.dec3 = UnetBlock(384,nc[-3],192)\n",
    "        self.dec2 = UnetBlock(192,nc[-4],96)\n",
    "        self.fpn = FPN([nc[-1],384,192],[32]*3)\n",
    "        self.drop = nn.Dropout2d(ps)\n",
    "        #self.final_conv = nn.Conv2d(96+32*3, num_classes, 3, padding=1)\n",
    "        #self.final_conv = nn.Sequential(UpBlock(96+32*3, 32),\n",
    "        #                                UpBlock(32, num_classes, blur=True))\n",
    "        self.final_conv = nn.Sequential(UpBlock(96+32*3, num_classes, blur=True))\n",
    "        self.up_result=1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        nt = x.shape[2]\n",
    "        x = x.permute(0,2,1,3,4).flatten(0,1)\n",
    "        encs = self.enc(x)\n",
    "        encs = [encs[k] for k in encs]\n",
    "        encs = [encs[0].view(-1,nt,*encs[0].shape[1:])[:,-1], \n",
    "                encs[1].view(-1,nt,*encs[1].shape[1:])[:,-1], \n",
    "                self.lstm[-2](encs[2].view(-1,nt,*encs[2].shape[1:]))[:,-1],\n",
    "                self.lstm[-1](encs[3].view(-1,nt,*encs[3].shape[1:]))[:,-1]]\n",
    "        dec4 = encs[-1]\n",
    "        dec3 = self.dec4(dec4,encs[-2])\n",
    "        dec2 = self.dec3(dec3,encs[-3])\n",
    "        dec1 = self.dec2(dec2,encs[-4])\n",
    "        x = self.fpn([dec4, dec3, dec2], dec1)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        if self.up_result != 0: x = F.interpolate(x,scale_factor=self.up_result,mode='bilinear')\n",
    "        return x\n",
    "    \n",
    "    split_layers = lambda model: \\\n",
    "                 (lambda m: [list(m.enc.parameters()),\n",
    "                 list(m.lstm.parameters()) + \n",
    "                 list(m.dec4.parameters()) + \n",
    "                 list(m.dec3.parameters()) + list(m.dec2.parameters()) +\n",
    "                 list(m.fpn.parameters()) +\n",
    "                 list(m.final_conv.parameters())]) \\\n",
    "                 (model if not isinstance(model, nn.DataParallel) else model.module)\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import math\n",
    "from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'p={}'.format(self.drop_prob)\n",
    "    \n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        # x = self.drop(x)\n",
    "        # commit this for the orignal BERT implement \n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim=16, M=10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.M = M\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.M) / half_dim\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))\n",
    "        emb = x[...,None] * emb[None,...]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "    \n",
    "#BEiTv2 block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm, **kwargs):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=drop, batch_first=True)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if init_values is not None:\n",
    "            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "        else:\n",
    "            self.gamma_1, self.gamma_2 = None, None\n",
    "\n",
    "    def forward(self, xq, xk, xv, attn_mask=None, key_padding_mask=None):\n",
    "        if self.gamma_1 is None:\n",
    "            x = xq + self.drop_path(self.attn(self.norm1(xq),self.norm1(xk),self.norm1(xv),\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        else:\n",
    "            x = xq + self.drop_path(self.gamma_1 * self.attn(self.norm1(xq),self.norm1(xk),self.norm1(xv),\n",
    "                            attn_mask=attn_mask,\n",
    "                            key_padding_mask=key_padding_mask,\n",
    "                            need_weights=False)[0])\n",
    "            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "class Tmixer(nn.Module):\n",
    "    def __init__(self, n, head_size=32, num_layers=2, **kwargs):\n",
    "        super().__init__()\n",
    "        self.seq_enc = SinusoidalPosEmb(n)\n",
    "        self.blocks = nn.ModuleList([Block(n,n//64) for i in range(num_layers)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        B,N,C,H,W = x.shape\n",
    "        x = x.flatten(-2,-1).permute(0,1,3,2)\n",
    "        \n",
    "        enc = self.seq_enc(torch.arange(N, device=x.device)).view(1,N,1,C)\n",
    "        xq = x[:,-1] + enc[:,-1]\n",
    "        xk = (x + enc).flatten(1,2)\n",
    "        xv = x.flatten(1,2)\n",
    "        \n",
    "        for m in self.blocks: xq = m(xq,xk,xv)\n",
    "        \n",
    "        x = xq.view(B,H,W,C).permute(0,3,1,2)\n",
    "        return x\n",
    "    \n",
    "class CoaT_ULSTM(nn.Module):\n",
    "    def __init__(self, pre='coat_lite_medium_a750cd63.pth', arch='medium', num_classes=1, ps=0, \n",
    "                 num_layers=2, **kwargs):\n",
    "        super().__init__()\n",
    "        if arch == 'mini': \n",
    "            self.enc = coat_lite_mini(return_interm_layers=True)\n",
    "            nc = [64,128,320,512]\n",
    "        elif arch == 'small': \n",
    "            self.enc = coat_lite_small(return_interm_layers=True)\n",
    "            nc = [64,128,320,512]\n",
    "        elif arch == 'medium': \n",
    "            self.enc = coat_lite_medium(return_interm_layers=True)\n",
    "            nc = [128,256,320,512]\n",
    "        else: raise Exception('Unknown model') \n",
    "        \n",
    "        if pre is not None:\n",
    "            sd = torch.load(pre)['model']\n",
    "            print(self.enc.load_state_dict(sd,strict=False))\n",
    "        \n",
    "        self.mixer = nn.ModuleList([Tmixer(nc[-2],num_layers=num_layers),\n",
    "                                    Tmixer(nc[-1],num_layers=num_layers)])\n",
    "        self.dec4 = UnetBlock(nc[-1],nc[-2],384)\n",
    "        self.dec3 = UnetBlock(384,nc[-3],192)\n",
    "        self.dec2 = UnetBlock(192,nc[-4],96)\n",
    "        self.fpn = FPN([nc[-1],384,192],[32]*3)\n",
    "        self.drop = nn.Dropout2d(ps)\n",
    "        #self.final_conv = nn.Conv2d(96+32*3, num_classes, 3, padding=1)\n",
    "        #self.final_conv = nn.Sequential(UpBlock(96+32*3, 32),\n",
    "        #                                UpBlock(32, num_classes, blur=True))\n",
    "        self.final_conv = nn.Sequential(UpBlock(96+32*3, num_classes, blur=True))\n",
    "        self.up_result=1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        nt = x.shape[2]\n",
    "        x = x.permute(0,2,1,3,4).flatten(0,1)\n",
    "        encs = self.enc(x)\n",
    "        encs = [encs[k] for k in encs]\n",
    "        encs = [encs[0].view(-1,nt,*encs[0].shape[1:])[:,-1], \n",
    "                encs[1].view(-1,nt,*encs[1].shape[1:])[:,-1], \n",
    "                self.mixer[-2](encs[2].view(-1,nt,*encs[2].shape[1:])),\n",
    "                self.mixer[-1](encs[3].view(-1,nt,*encs[3].shape[1:]))]\n",
    "        dec4 = encs[-1]\n",
    "        dec3 = self.dec4(dec4,encs[-2])\n",
    "        dec2 = self.dec3(dec3,encs[-3])\n",
    "        dec1 = self.dec2(dec2,encs[-4])\n",
    "        x = self.fpn([dec4, dec3, dec2], dec1)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        if self.up_result != 0: x = F.interpolate(x,scale_factor=self.up_result,mode='bilinear')\n",
    "        return x\n",
    "    \n",
    "    split_layers = lambda model: \\\n",
    "                 (lambda m: [list(m.enc.parameters()),\n",
    "                 list(m.mixer.parameters()) + \n",
    "                 list(m.dec4.parameters()) + \n",
    "                 list(m.dec3.parameters()) + list(m.dec2.parameters()) +\n",
    "                 list(m.fpn.parameters()) +\n",
    "                 list(m.final_conv.parameters())]) \\\n",
    "                 (model if not isinstance(model, nn.DataParallel) else model.module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f0ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcae11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUT = 'experiments'\n",
    "fname = 'Seq_CoaT_512_1'\n",
    "\n",
    "for fold in range(1):\n",
    "    ds_train = ContrailsDataset(CONFIG.path, train=True, tfms=get_aug())\n",
    "    ds_val = ContrailsDataset(CONFIG.path, train=False, tfms=None)\n",
    "    \n",
    "    model = CoaT_ULSTM().cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    data = ImageDataLoaders.from_dsets(ds_train,\n",
    "                                   ds_val,\n",
    "                                   bs=CONFIG.bs,\n",
    "                                   num_workers=CONFIG.num_workers,\n",
    "                                   pin_memory=True\n",
    "                                  ).cuda()\n",
    "\n",
    "    learn = Learner(data, \n",
    "                    model,\n",
    "                    path = CONFIG.out, \n",
    "                    loss_func=loss_comb,\n",
    "                    metrics=[F_th()],\n",
    "                    cbs=[\n",
    "                    GradientClip(3.0),\n",
    "                    GradientAccumulation(32//CONFIG.bs),\n",
    "                    CSVLogger(),\n",
    "                    SaveModelCallback(monitor='f_th'),\n",
    "                    ],\n",
    "                    opt_func=partial(WrapperOver9000,eps=1e-4),\n",
    "                    #opt_func=partial(WrapperAdamW,eps=1e-4),\n",
    "                    #splitter = SwinFormer.split_layers\n",
    "                   ).to_fp16()\n",
    "    \n",
    "    learn.fit_one_cycle(24, lr_max=3.5e-4, pct_start=0.1)\n",
    "    torch.save(learn.model.module.state_dict(),os.path.join(OUT,f'{fname}_{fold}.pth'))\n",
    "    \n",
    "    #del learn, data, ds_train, ds_val\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f20d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = learn.metrics[0]\n",
    "dices = (1 + metric.beta**2)*metric.tp/\\\n",
    "        ((1 + metric.beta**2)*metric.tp + metric.beta**2*metric.fn + metric.fp + 1e-6)\n",
    "ths = metric.ths\n",
    "\n",
    "best_dice = dices.max()\n",
    "best_thr = ths[dices.argmax()]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(ths, dices, color='blue')\n",
    "plt.vlines(x=best_thr, ymin=dices.min(), ymax=dices.max(), colors='black')\n",
    "d = dices.max() - dices.min()\n",
    "plt.text(ths[-1]-0.1, best_dice-0.1*d, f'DICE = {best_dice:.3f}', fontsize=12);\n",
    "plt.text(ths[-1]-0.1, best_dice-0.2*d, f'TH = {best_thr:.3f}', fontsize=12);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1971b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformer mixer, 4 layers\n",
    "OUT = 'experiments'\n",
    "fname = 'Seq_CoaT_512_2'\n",
    "\n",
    "for fold in range(1):\n",
    "    ds_train = ContrailsDataset(CONFIG.path, train=True, tfms=get_aug())\n",
    "    ds_val = ContrailsDataset(CONFIG.path, train=False, tfms=None)\n",
    "    \n",
    "    model = CoaT_ULSTM(num_layers=4).cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    data = ImageDataLoaders.from_dsets(ds_train,\n",
    "                                   ds_val,\n",
    "                                   bs=CONFIG.bs,\n",
    "                                   num_workers=CONFIG.num_workers,\n",
    "                                   pin_memory=True\n",
    "                                  ).cuda()\n",
    "\n",
    "    learn = Learner(data, \n",
    "                    model,\n",
    "                    path = CONFIG.out, \n",
    "                    loss_func=loss_comb,\n",
    "                    metrics=[F_th()],\n",
    "                    cbs=[\n",
    "                    GradientClip(3.0),\n",
    "                    GradientAccumulation(32//CONFIG.bs),\n",
    "                    CSVLogger(),\n",
    "                    SaveModelCallback(monitor='f_th'),\n",
    "                    ],\n",
    "                    opt_func=partial(WrapperOver9000,eps=1e-4),\n",
    "                    #opt_func=partial(WrapperAdamW,eps=1e-4),\n",
    "                    #splitter = SwinFormer.split_layers\n",
    "                   ).to_fp16()\n",
    "    \n",
    "    learn.fit_one_cycle(24, lr_max=3.5e-4, pct_start=0.1)\n",
    "    torch.save(learn.model.module.state_dict(),os.path.join(OUT,f'{fname}_{fold}.pth'))\n",
    "    \n",
    "    #del learn, data, ds_train, ds_val\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee37dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a590a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc494865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48bcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd91940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ecdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c9a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f08243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d28c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b42331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
